{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb97e514-58a0-4125-a6e0-325cae5748bb",
   "metadata": {},
   "source": [
    "# Harmonising Data with the Ocean Information Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efa20d-71a4-437b-bbf1-258ab19a658a",
   "metadata": {},
   "source": [
    "In this example notebook, we demonstrate how to ingest data into the Ocean Data Platform using the Ocean Information Model harmonisation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90961588-0d0b-4707-9551-ffcb1de04859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from os.path import basename\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import datetime, time, threading, json\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import json\n",
    "import uuid\n",
    "from odp.client import OdpClient\n",
    "from odp.dto.catalog import  DatasetDto\n",
    "from odp.client.dto.file_dto import FileMetadataDto\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271010b-2309-44b5-abd0-f0623b7c7ec3",
   "metadata": {},
   "source": [
    "## Prepare data for OIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778f9589-bb22-4e40-87a8-1eda522a358f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 16.10.2024-OIM-updated-sites.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "# Define file names\n",
    "input_file = '16.10.2024-OIM-updated-sites.xlsx'\n",
    "output_file = '16.10.2024-OIM-updated-sites.csv'\n",
    "\n",
    "# Read the first sheet of the Excel file\n",
    "df = pd.read_excel(input_file, sheet_name=0)  # `sheet_name=0` refers to the first tab\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Saved {output_file} successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0484fd-dd14-4e81-a69b-f8b8f1cebb77",
   "metadata": {},
   "source": [
    "## OIM Harmonisation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d425e8a-89e2-424b-b444-f2650cc9eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-15 15:00:26 ==> zipfile: input.zip\n",
      "2024-11-15 15:00:27 ==> finallink: https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/access/files/003d9dda-ea2b-4d2a-8fef-e288117b3cb2/download\n",
      "2024-11-15 15:00:29 ==> jobid: cdade8ac-1f81-4c3c-94d2-2dfcdbab71b1\n",
      "2024-11-15 15:00:29 ==> sleeping: PENDING\n",
      "2024-11-15 15:00:36 ==> sleeping: PENDING\n",
      "2024-11-15 15:00:44 ==> sleeping: PENDING\n",
      "2024-11-15 15:00:52 ==> sleeping: PENDING\n",
      "2024-11-15 15:00:59 ==> sleeping: PENDING\n",
      "2024-11-15 15:01:07 ==> sleeping: PENDING\n",
      "2024-11-15 15:01:15 ==> sleeping: PENDING\n",
      "2024-11-15 15:01:15 ==> job finished with status: SUCCESS\n",
      "2024-11-15 15:01:22 ==> results_url: https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/generic/3d377d87-960c-4cac-84e1-54f2e31d944d/download\n",
      "2024-11-15 15:01:22 ==> Finished. Harmonized data available in file: 16.10.2024-OIM-updated-sites.jsonld\n"
     ]
    }
   ],
   "source": [
    "def dpi_get_token():\n",
    "    ## get token\n",
    "    data = {\n",
    "        'client_id': 'dpi-enabler-cli',\n",
    "        'client_secret': 'U7hbMiInBTACDArOchhd11dzsL076mEq',\n",
    "        'username': 'rpalma@man.poznan.pl',\n",
    "        'password': 'ttoeuc',\n",
    "        'grant_type': 'password'\n",
    "    }\n",
    "    response = requests.post('https://keycloak-dev.apps.paas-dev.psnc.pl/auth/realms/demeter/protocol/openid-connect/token', data=data)\n",
    "    #print(json.dumps(response.json(), indent=2))\n",
    "    return response.json()['access_token']\n",
    "\n",
    "def upload_to_dpi(fullfilename):\n",
    "    path=os.path.dirname(os.path.abspath(fullfilename))\n",
    "    #print(\"upload_to_dpi fullfilename: \" + fullfilename)\n",
    "    #print(\"upload_to_dpi path: \"+ path)\n",
    "\n",
    "    ## get token\n",
    "    dpitoken=dpi_get_token()\n",
    "    \n",
    "    ## upload file to DPI and get shareable link\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer '+dpitoken,\n",
    "        'accept': 'application/json',\n",
    "        #'Content-Type': 'application/zip'\n",
    "    }\n",
    "    data = { \"description\": \"input.zip\"}\n",
    "    files = {    \n",
    "        'file': (os.path.join(path, \"input.zip\"), open(os.path.join(path, \"input.zip\"), 'rb')),\n",
    "    }\n",
    "    response = requests.post('https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/access/files', headers=headers, files=files, data=data)\n",
    "    if response.ok:\n",
    "        #print(\"time to execute api call to upload file to dpi: \"+str(response.elapsed.total_seconds()))\n",
    "        #print(json.dumps(response.json(), indent=2))\n",
    "        sharelink=response.json()['output_url']        \n",
    "        #delete zip file\n",
    "        if os.path.exists(os.path.join(path, \"input.zip\")):\n",
    "            os.remove(os.path.join(path, \"input.zip\"))\n",
    "        return sharelink\n",
    "    else: \n",
    "        print (response)\n",
    "        return None \n",
    "\n",
    "def execute_pipeline(pipelineid,graph,sharelink):    \n",
    "    ## get token\n",
    "    dpitoken=dpi_get_token()\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer '+dpitoken,\n",
    "        'accept': 'application/json',        \n",
    "    }\n",
    "\n",
    "    data = { \n",
    "        \"preprocess\": True,\n",
    "        \"mapping\": True,\n",
    "        \"transform\": True,\n",
    "        \"postprocess\": True,\n",
    "        \"load\": True,\n",
    "        \"link\": False,\n",
    "        \"input_type\": \"CSV\",\n",
    "        \"preprocess_activities\":  [ \"normalize_delimiter\" ],\n",
    "        \"graph_uri\": graph, \n",
    "        \"url_input\": sharelink #\"https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/access/files/3d744c2f-aa90-41a2-b666-e95da7130c9f/download\",        \n",
    "    }\n",
    "    response = requests.patch('https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/generic/'+pipelineid, headers=headers, data=data)\n",
    "    if response.ok:\n",
    "        #print(\"time to execute api call to update pipeline: \"+str(response.elapsed.total_seconds()))        \n",
    "        #print(json.dumps(response.json(), indent=2))  \n",
    "\n",
    "        ## finally execute it\n",
    "        response = requests.post('https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/generic/'+pipelineid+'/execute', headers=headers)\n",
    "        if response.ok:\n",
    "            #print(\"time to execute api call to execute pipeline: \"+str(response.elapsed.total_seconds()))                    \n",
    "            #print(json.dumps(response.json(), indent=2))\n",
    "            jobid=response.json()['identifier']\n",
    "            return jobid\n",
    "        else:\n",
    "            print (response)\n",
    "            return None\n",
    "    else:\n",
    "        print (response)\n",
    "        return None\n",
    "\n",
    "def get_results_url(jobid):\n",
    "    job_url = 'https://dpi-enabler-demeter.apps.paas-dev.psnc.pl/api/jobs/'+str(jobid)    \n",
    "    \n",
    "    response = None\n",
    "    status = 'PENDING'\n",
    "    while status == 'PENDING':\n",
    "        ## sleep 7 sec\n",
    "        now = datetime.datetime.now()\n",
    "        print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"sleeping: \"+status)                      \n",
    "        time.sleep(7.0)\n",
    "        ## get token\n",
    "        dpitoken=dpi_get_token()    \n",
    "        ## upload file to DPI and get shareable link\n",
    "        headers = {\n",
    "            'Authorization': 'Bearer '+dpitoken,\n",
    "            'accept': 'application/json',            \n",
    "        }\n",
    "        response = requests.get(job_url, headers=headers)        \n",
    "        if response.ok:            \n",
    "            status=response.json()['status']            \n",
    "        else:\n",
    "            status=\"ERROR\"        \n",
    "\n",
    "    print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"job finished with status: \"+status)        \n",
    "    if response.ok:            \n",
    "        output_url=response.json()['results']            \n",
    "        return output_url\n",
    "    else: \n",
    "        print (response)\n",
    "        return None    \n",
    "\n",
    "def download_harmonized_data_file (graph_uri, harmonized_data_file):\n",
    "    headers = {\n",
    "        'Accept': 'application/ld+json',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    }    \n",
    "    data = {\n",
    "        'query': 'CONSTRUCT { ?s ?p ?o } WHERE {GRAPH <'+graph_uri+'> { ?s ?p ?o } }',\n",
    "    }\n",
    "    response = requests.post('https://www.foodie-cloud.org/sparql', headers=headers, data=data)\n",
    "    with open(harmonized_data_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "######## start execution #######\n",
    "\n",
    "### input variables\n",
    "input = '16.10.2024-OIM-updated-sites.csv' ### this is the CSV file you get in ODP from the input excel for the tab sites\n",
    "pipeline_id = \"3d377d87-960c-4cac-84e1-54f2e31d944d\"\n",
    "graph_uri = \"https://w3id.org/iliad/israel/culturalHeritage\"\n",
    "zfile='input.zip'\n",
    "\n",
    "### zip input CSV\n",
    "with ZipFile(zfile, 'w') as zipf:\n",
    "    zipf.write(input, arcname=os.path.basename(input))\n",
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"zipfile: \"+zfile)      \n",
    "\n",
    "### upload zip input CSV file (and remove it)\n",
    "finallink = upload_to_dpi (zfile)\n",
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"finallink: \"+finallink)      \n",
    "\n",
    "### execute pipeline\n",
    "jobid = execute_pipeline(pipeline_id,graph_uri,finallink)                       \n",
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"jobid: \"+jobid)      \n",
    "\n",
    "### wait for job to finish\n",
    "results_url=get_results_url(jobid)\n",
    "now = datetime.datetime.now()\n",
    "if results_url!=None:\n",
    "    print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"results_url: \"+results_url)      \n",
    "    ### get harmonized data\n",
    "    harmonized_data_file = input.replace('.csv','.jsonld')\n",
    "    download_harmonized_data_file (graph_uri, harmonized_data_file)\n",
    "    print(now.strftime(\"%Y-%m-%d %H:%M:%S ==> \")+\"Finished. Harmonized data available in file: \"+harmonized_data_file) ### this is the harmonized data named as input file chaning extension from csv to jsonld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d0128-fa73-4a87-82fa-f9d922209198",
   "metadata": {},
   "source": [
    "## Ingest to ODP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ff8c2-f216-4058-91df-ec628130bce9",
   "metadata": {},
   "source": [
    "### Create resource in ODP Catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa2868d-33ea-4aa3-a7ee-5f0a515376ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OdpClient()\n",
    "uuid_str = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c981e1-f599-4411-9f3d-62c60fc92115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove first row of csv data\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('16.10.2024-OIM-updated-sites.csv')\n",
    "\n",
    "# Remove the first row (index 0)\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# Optionally, save the modified DataFrame back to a CSV file\n",
    "df.to_csv('16.10.2024-OIM-updated-sites_odp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73610c7-cfb8-4392-b739-5186d0497880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collection\n",
    "with open(\"collection.json\") as file:\n",
    "    collection_config = json.load(file)\n",
    "\n",
    "# randomize collection name:\n",
    "collection_config[\"metadata\"][\"name\"] = collection_config[\"metadata\"][\"name\"] + \"-\" + uuid_str\n",
    "\n",
    "collection_manifest = DataCollectionDto(**collection_config)\n",
    "collection_dto = client.catalog.create(collection_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6a2c4-7668-44ce-a62e-bc0530a5fd76",
   "metadata": {},
   "source": [
    "### Load data into to ODP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18063992-a4d6-42a4-a9a6-9b078c0af6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config ={\n",
    "        \"dataset_config\": {\n",
    "            \"kind\": \"catalog.hubocean.io/dataset\",\n",
    "            \"version\": \"v1alpha3\",\n",
    "            \"metadata\": {\n",
    "                \"name\": \"iliad-cultural-heritage-oim\",\n",
    "                \"display_name\": \"Iliad Underwater Cultural Heritage Sites- OIM\",\n",
    "                \"description\": \"A collection of underwater cultural heritage sites in Isreal, harmonised through OIM.\",\n",
    "                \"labels\": {\n",
    "                    \"catalog.hubocean.io/internal\": True\n",
    "                }\n",
    "            },\n",
    "            \"spec\": {\n",
    "                \"storage_class\": \"registry.hubocean.io/storageClass/raw\",\n",
    "                \"storage_controller\": \"registry.hubocean.io/storageController/storage-raw-cdffs\",\n",
    "                \"data_collection\": \"catalog.hubocean.io/dataCollection/iliad_cultural_heritage-af7f917c-6168-4e2a-8b1e-c6107ee14394\", # ensure the same as above\n",
    "                \"maintainer\": {\n",
    "                    \"contact\": \"Ehud Galili <galiliudi@gmail.com>\",\n",
    "                    \"organisation\": \"University of Haifa\"\n",
    "                },\n",
    "                \"citation\": None,\n",
    "                \"documentation\": [],\n",
    "                \"facets\": None,\n",
    "                \"tags\": [\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "file_config = {\n",
    "    \"local_file_path\": \"16.10.2024-OIM-updated-sites.jsonld\", # output of OIM\n",
    "    \"mime_type\": \"application/ld+json\",\n",
    "    \"metadata\": {}\n",
    "}\n",
    "\n",
    "client = OdpClient()\n",
    "dataset_manifest = DatasetDto(**dataset_config[\"dataset_config\"])\n",
    "dataset_dto = client.catalog.create(dataset_manifest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907b63cd-a80e-45a0-bf15-64d4d3b010ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileMetadataDto(name='16.10.2024-OIM-updated-sites.jsonld', mime_type='application/ld+json', dataset=UUID('024a2bd2-e050-4d26-ade0-c6049f998c55'), metadata={'hubocean.io/app': 'odcat', 'hubocean.io/dataset': '024a2bd2-e050-4d26-ade0-c6049f998c55'}, geo_location=None, size_bytes=2008082, checksum='9b6455823df87d2867f81395256068e9', created_time=datetime.datetime(2024, 11, 15, 15, 16, 34, 248000), modified_time=datetime.datetime(2024, 11, 15, 15, 16, 34, 856000), deleted_time=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_path = file_config[\"local_file_path\"]\n",
    "mime_type = file_config[\"mime_type\"]\n",
    "metadata = file_config[\"metadata\"]\n",
    "geo_location = file_config.get(\"geo_location\")\n",
    "file_name = local_file_path.split(\"/\")[-1]\n",
    "with open(local_file_path, \"rb\") as file:\n",
    "    byte_content = file.read()\n",
    "client.raw.create_file(\n",
    "    resource_dto=dataset_dto,\n",
    "    file_metadata_dto=FileMetadataDto(\n",
    "        **{\n",
    "            \"name\": file_name,\n",
    "            \"mime_type\": mime_type,\n",
    "            \"metadata\": metadata,\n",
    "            \"geo_location\": geo_location,\n",
    "        }\n",
    "    ),\n",
    "    contents=byte_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30552a-c1d7-496d-b2f8-a9a0fc4cd87d",
   "metadata": {},
   "source": [
    "## Add tabulr upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf7a8a-c1c9-4332-9955-ff8803113978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
